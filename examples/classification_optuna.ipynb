{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/TorkamaniLab/lohrasb.git\n",
      "  Cloning https://github.com/TorkamaniLab/lohrasb.git to /private/var/folders/v1/xbcjnd1x5rn7ct1m_rnsblk80000gp/T/pip-req-build-8ezxq23l\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/TorkamaniLab/lohrasb.git /private/var/folders/v1/xbcjnd1x5rn7ct1m_rnsblk80000gp/T/pip-req-build-8ezxq23l\n",
      "  Resolved https://github.com/TorkamaniLab/lohrasb.git to commit 5216eb6563dc3152c5cc6d44d2488a8c614ccb80\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting aiosignal==1.3.1 (from lohrasb==4.1.0)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting alembic==1.12.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for alembic==1.12.0 from https://files.pythonhosted.org/packages/a2/8b/46919127496036c8e990b2b236454a0d8655fd46e1df2fd35610a9cbc842/alembic-1.12.0-py3-none-any.whl.metadata\n",
      "  Using cached alembic-1.12.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting argcomplete==3.1.1 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for argcomplete==3.1.1 from https://files.pythonhosted.org/packages/4f/ef/8b604222ba5e5190e25851aa3a5b754f2002361dc62a258a8e9f13e866f4/argcomplete-3.1.1-py3-none-any.whl.metadata\n",
      "  Using cached argcomplete-3.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting attrs==23.1.0 (from lohrasb==4.1.0)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting black==23.3.0 (from lohrasb==4.1.0)\n",
      "  Using cached black-23.3.0-cp37-cp37m-macosx_10_16_x86_64.whl (1.4 MB)\n",
      "Collecting catboost==1.2.1 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for catboost==1.2.1 from https://files.pythonhosted.org/packages/c1/bb/ea618c1192f926dcf51228eabe073695e9acede9d00205d0bd14bbba0654/catboost-1.2.1-cp37-cp37m-macosx_11_0_universal2.whl.metadata\n",
      "  Using cached catboost-1.2.1-cp37-cp37m-macosx_11_0_universal2.whl.metadata (1.2 kB)\n",
      "Collecting certifi==2023.7.22 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for certifi==2023.7.22 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer==3.2.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for charset-normalizer==3.2.0 from https://files.pythonhosted.org/packages/95/ee/8bb03c3518a228dc5956d1b4f46d8258639ff118881fba456b72b06561cf/charset_normalizer-3.2.0-cp37-cp37m-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached charset_normalizer-3.2.0-cp37-cp37m-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Collecting click==8.1.7 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for click==8.1.7 from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpickle==2.2.1 (from lohrasb==4.1.0)\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting cmaes==0.10.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for cmaes==0.10.0 from https://files.pythonhosted.org/packages/f7/46/7d9544d453346f6c0c405916c95fdb653491ea2e9976cabb810ba2fe8cd4/cmaes-0.10.0-py3-none-any.whl.metadata\n",
      "  Using cached cmaes-0.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting colorlog==6.7.0 (from lohrasb==4.1.0)\n",
      "  Using cached colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting cycler==0.11.0 (from lohrasb==4.1.0)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting dill==0.3.7 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for dill==0.3.7 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting distlib==0.3.7 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for distlib==0.3.7 from https://files.pythonhosted.org/packages/43/a0/9ba967fdbd55293bacfc1507f58e316f740a3b231fc00e3d86dc39bc185a/distlib-0.3.7-py2.py3-none-any.whl.metadata\n",
      "  Using cached distlib-0.3.7-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting exceptiongroup==1.1.3 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for exceptiongroup==1.1.3 from https://files.pythonhosted.org/packages/ad/83/b71e58666f156a39fb29417e4c8ca4bc7400c0dd4ed9e8842ab54dc8c344/exceptiongroup-1.1.3-py3-none-any.whl.metadata\n",
      "  Using cached exceptiongroup-1.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting filelock==3.12.2 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for filelock==3.12.2 from https://files.pythonhosted.org/packages/00/45/ec3407adf6f6b5bf867a4462b2b0af27597a26bd3cd6e2534cb6ab029938/filelock-3.12.2-py3-none-any.whl.metadata\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting fonttools==4.38.0 (from lohrasb==4.1.0)\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Collecting frozenlist==1.3.3 (from lohrasb==4.1.0)\n",
      "  Using cached frozenlist-1.3.3-cp37-cp37m-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting future==0.18.3 (from lohrasb==4.1.0)\n",
      "  Using cached future-0.18.3-py3-none-any.whl\n",
      "Collecting graphviz==0.20.1 (from lohrasb==4.1.0)\n",
      "  Using cached graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Collecting greenlet==2.0.2 (from lohrasb==4.1.0)\n",
      "  Using cached greenlet-2.0.2-cp37-cp37m-macosx_10_15_x86_64.whl (240 kB)\n",
      "Collecting grpcio==1.57.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for grpcio==1.57.0 from https://files.pythonhosted.org/packages/a6/f9/b9c6e8cbfe132d2561bf56f66f926365f3eb2aac4fd1cf4c1356a954dc7e/grpcio-1.57.0-cp37-cp37m-macosx_10_10_universal2.whl.metadata\n",
      "  Using cached grpcio-1.57.0-cp37-cp37m-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting hyperopt==0.2.7 (from lohrasb==4.1.0)\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting idna==3.4 (from lohrasb==4.1.0)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting imbalanced-learn==0.11.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for imbalanced-learn==0.11.0 from https://files.pythonhosted.org/packages/a3/9e/fbe60a768502af54563dcb59ca7856f5a8833b3ad5ada658922e1ab09b7f/imbalanced_learn-0.11.0-py3-none-any.whl.metadata\n",
      "  Using cached imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting imblearn==0.0 (from lohrasb==4.1.0)\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting importlib-metadata==6.7.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for importlib-metadata==6.7.0 from https://files.pythonhosted.org/packages/ff/94/64287b38c7de4c90683630338cf28f129decbba0a44f0c6db35a873c73c4/importlib_metadata-6.7.0-py3-none-any.whl.metadata\n",
      "  Using cached importlib_metadata-6.7.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting importlib-resources==5.12.0 (from lohrasb==4.1.0)\n",
      "  Using cached importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting iniconfig==2.0.0 (from lohrasb==4.1.0)\n",
      "  Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting isort==5.11.5 (from lohrasb==4.1.0)\n",
      "  Using cached isort-5.11.5-py3-none-any.whl (104 kB)\n",
      "Collecting joblib==1.3.2 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for joblib==1.3.2 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting jsonschema==4.17.3 (from lohrasb==4.1.0)\n",
      "  Using cached jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Collecting kiwisolver==1.4.5 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for kiwisolver==1.4.5 from https://files.pythonhosted.org/packages/90/54/d173ef7c814476f23471781768804356494363a824e312a1bd0fef50344c/kiwisolver-1.4.5-cp37-cp37m-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached kiwisolver-1.4.5-cp37-cp37m-macosx_10_9_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting lightgbm==4.0.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for lightgbm==4.0.0 from https://files.pythonhosted.org/packages/7c/72/ad920d3ce872cb5cb1778930988418f758cf17e7dd3c59e7457c6960c057/lightgbm-4.0.0-py3-none-macosx_10_15_x86_64.macosx_11_6_x86_64.macosx_12_5_x86_64.whl.metadata\n",
      "  Using cached lightgbm-4.0.0-py3-none-macosx_10_15_x86_64.macosx_11_6_x86_64.macosx_12_5_x86_64.whl.metadata (19 kB)\n",
      "Collecting Mako==1.2.4 (from lohrasb==4.1.0)\n",
      "  Using cached Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "Collecting MarkupSafe==2.1.3 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for MarkupSafe==2.1.3 from https://files.pythonhosted.org/packages/a8/12/fd9ef3e09a7312d60467c71037283553ff2acfcd950159cd4c3ca9558af4/MarkupSafe-2.1.3-cp37-cp37m-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached MarkupSafe-2.1.3-cp37-cp37m-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting matplotlib==3.5.3 (from lohrasb==4.1.0)\n",
      "  Using cached matplotlib-3.5.3-cp37-cp37m-macosx_10_9_x86_64.whl (7.3 MB)\n",
      "Collecting mccabe==0.7.0 (from lohrasb==4.1.0)\n",
      "  Using cached mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting msgpack==1.0.5 (from lohrasb==4.1.0)\n",
      "  Using cached msgpack-1.0.5-cp37-cp37m-macosx_10_9_x86_64.whl (72 kB)\n",
      "Collecting mypy-extensions==1.0.0 (from lohrasb==4.1.0)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting networkx==2.6.3 (from lohrasb==4.1.0)\n",
      "  Using cached networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "Collecting nox==2023.4.22 (from lohrasb==4.1.0)\n",
      "  Using cached nox-2023.4.22-py3-none-any.whl (54 kB)\n",
      "Collecting numpy==1.21.6 (from lohrasb==4.1.0)\n",
      "  Using cached numpy-1.21.6-cp37-cp37m-macosx_10_9_x86_64.whl (16.9 MB)\n",
      "Collecting optuna==3.3.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for optuna==3.3.0 from https://files.pythonhosted.org/packages/69/60/87a06ef66b34cbe2f2eb0ab66f003664404a7f40c21403a69fad7e28a82b/optuna-3.3.0-py3-none-any.whl.metadata\n",
      "  Using cached optuna-3.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting packaging==23.1 (from lohrasb==4.1.0)\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Collecting pandas==1.3.5 (from lohrasb==4.1.0)\n",
      "  Using cached pandas-1.3.5-cp37-cp37m-macosx_10_9_x86_64.whl (11.0 MB)\n",
      "Collecting pathspec==0.11.2 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for pathspec==0.11.2 from https://files.pythonhosted.org/packages/b4/2a/9b1be29146139ef459188f5e420a66e835dda921208db600b7037093891f/pathspec-0.11.2-py3-none-any.whl.metadata\n",
      "  Using cached pathspec-0.11.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Pillow==9.5.0 (from lohrasb==4.1.0)\n",
      "  Using cached Pillow-9.5.0-cp37-cp37m-macosx_10_10_x86_64.whl (3.4 MB)\n",
      "Collecting pkgutil_resolve_name==1.3.10 (from lohrasb==4.1.0)\n",
      "  Using cached pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Collecting platformdirs==3.10.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for platformdirs==3.10.0 from https://files.pythonhosted.org/packages/14/51/fe5a0d6ea589f0d4a1b97824fb518962ad48b27cd346dcdfa2405187997a/platformdirs-3.10.0-py3-none-any.whl.metadata\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting plotly==5.16.1 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for plotly==5.16.1 from https://files.pythonhosted.org/packages/26/5d/1e13b597ed8e54803e9ac6ded18c04cd35d8cbc49016778ec50c4ca9e9d5/plotly-5.16.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached plotly-5.16.1-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pluggy==1.2.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for pluggy==1.2.0 from https://files.pythonhosted.org/packages/51/32/4a79112b8b87b21450b066e102d6608907f4c885ed7b04c3fdb085d4d6ae/pluggy-1.2.0-py3-none-any.whl.metadata\n",
      "  Using cached pluggy-1.2.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting protobuf==4.24.2 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for protobuf==4.24.2 from https://files.pythonhosted.org/packages/ac/8f/a7e5dfc2d285526c74b82f118d5b4857875f39405aa1d6f1df56ef25a070/protobuf-4.24.2-cp37-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Using cached protobuf-4.24.2-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\n",
      "Collecting py4j==0.10.9.7 (from lohrasb==4.1.0)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Collecting pyarrow==12.0.1 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for pyarrow==12.0.1 from https://files.pythonhosted.org/packages/8f/56/10fab8ea743b9bfd954d8648e715e1a947d7e131858d9670f83770626059/pyarrow-12.0.1-cp37-cp37m-macosx_10_14_x86_64.whl.metadata\n",
      "  Using cached pyarrow-12.0.1-cp37-cp37m-macosx_10_14_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pycodestyle==2.9.1 (from lohrasb==4.1.0)\n",
      "  Using cached pycodestyle-2.9.1-py2.py3-none-any.whl (41 kB)\n",
      "Collecting pyflakes==2.5.0 (from lohrasb==4.1.0)\n",
      "  Using cached pyflakes-2.5.0-py2.py3-none-any.whl (66 kB)\n",
      "Collecting pyparsing==3.1.1 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for pyparsing==3.1.1 from https://files.pythonhosted.org/packages/39/92/8486ede85fcc088f1b3dba4ce92dd29d126fd96b0008ea213167940a2475/pyparsing-3.1.1-py3-none-any.whl.metadata\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pyrsistent==0.19.3 (from lohrasb==4.1.0)\n",
      "  Using cached pyrsistent-0.19.3-cp37-cp37m-macosx_10_9_x86_64.whl (69 kB)\n",
      "Collecting pytest==7.4.1 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for pytest==7.4.1 from https://files.pythonhosted.org/packages/78/af/1a79db43409ea8569a8a91d0a87db4445c7de4cefcf6141e9a5c77dda2d6/pytest-7.4.1-py3-none-any.whl.metadata\n",
      "  Using cached pytest-7.4.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting python-dateutil==2.8.2 (from lohrasb==4.1.0)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting python-dotenv==0.21.1 (from lohrasb==4.1.0)\n",
      "  Using cached python_dotenv-0.21.1-py3-none-any.whl (19 kB)\n",
      "Collecting pytz==2023.3 (from lohrasb==4.1.0)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting PyYAML==6.0.1 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for PyYAML==6.0.1 from https://files.pythonhosted.org/packages/c7/d1/02baa09d39b1bb1ebaf0d850d106d1bdcb47c91958557f471153c49dc03b/PyYAML-6.0.1-cp37-cp37m-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached PyYAML-6.0.1-cp37-cp37m-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting ray==2.6.3 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for ray==2.6.3 from https://files.pythonhosted.org/packages/e8/0e/f50f50e5deb640d236550d9af4cf27fa1e83263fc498c4e3388c020860ed/ray-2.6.3-cp37-cp37m-macosx_10_15_x86_64.whl.metadata\n",
      "  Using cached ray-2.6.3-cp37-cp37m-macosx_10_15_x86_64.whl.metadata (12 kB)\n",
      "Collecting requests==2.31.0 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for requests==2.31.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting ruamel.yaml==0.17.32 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for ruamel.yaml==0.17.32 from https://files.pythonhosted.org/packages/d9/0e/2a05efa11ea33513fbdf4a2e2576fe94fd8fa5ad226dbb9c660886390974/ruamel.yaml-0.17.32-py3-none-any.whl.metadata\n",
      "  Using cached ruamel.yaml-0.17.32-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting ruamel.yaml.clib==0.2.7 (from lohrasb==4.1.0)\n",
      "  Using cached ruamel.yaml.clib-0.2.7-cp37-cp37m-macosx_10_9_x86_64.whl (140 kB)\n",
      "Collecting scikit-learn==1.0.2 (from lohrasb==4.1.0)\n",
      "  Using cached scikit_learn-1.0.2-cp37-cp37m-macosx_10_13_x86_64.whl (7.8 MB)\n",
      "Collecting scipy==1.7.3 (from lohrasb==4.1.0)\n",
      "  Using cached scipy-1.7.3-cp37-cp37m-macosx_10_9_x86_64.whl (33.0 MB)\n",
      "Collecting six==1.16.0 (from lohrasb==4.1.0)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting SQLAlchemy==2.0.20 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for SQLAlchemy==2.0.20 from https://files.pythonhosted.org/packages/80/7f/be3eaa10c45dedcb8eb6924c1bf9a4998e154f25d85402f6313dab138e9d/SQLAlchemy-2.0.20-cp37-cp37m-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached SQLAlchemy-2.0.20-cp37-cp37m-macosx_10_9_x86_64.whl.metadata (9.4 kB)\n",
      "Collecting tenacity==8.2.3 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for tenacity==8.2.3 from https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl.metadata\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting tensorboardX==2.6.2.2 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for tensorboardX==2.6.2.2 from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting threadpoolctl==3.1.0 (from lohrasb==4.1.0)\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting tomli==2.0.1 (from lohrasb==4.1.0)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting tqdm==4.66.1 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for tqdm==4.66.1 from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting tune-sklearn==0.4.6 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for tune-sklearn==0.4.6 from https://files.pythonhosted.org/packages/43/10/bb45f851c5e7324cfe711336099a99db79100a8f8f9aceb58c5693ecf106/tune_sklearn-0.4.6-py3-none-any.whl.metadata\n",
      "  Using cached tune_sklearn-0.4.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typed-ast==1.5.5 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for typed-ast==1.5.5 from https://files.pythonhosted.org/packages/d5/00/635353c31b71ed307ab020eff6baed9987da59a1b2ba489f885ecbe293b8/typed_ast-1.5.5-cp37-cp37m-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached typed_ast-1.5.5-cp37-cp37m-macosx_10_9_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting typing_extensions==4.7.1 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for typing_extensions==4.7.1 from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting urllib3==2.0.4 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for urllib3==2.0.4 from https://files.pythonhosted.org/packages/9b/81/62fd61001fa4b9d0df6e31d47ff49cfa9de4af03adecf339c7bc30656b37/urllib3-2.0.4-py3-none-any.whl.metadata\n",
      "  Using cached urllib3-2.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting virtualenv==20.24.4 (from lohrasb==4.1.0)\n",
      "  Obtaining dependency information for virtualenv==20.24.4 from https://files.pythonhosted.org/packages/48/87/0ff871ebe003075d61e1abeab67c21d50edf44dbfdeabd107bef30a9e027/virtualenv-20.24.4-py3-none-any.whl.metadata\n",
      "  Using cached virtualenv-20.24.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting xgboost==1.6.2 (from lohrasb==4.1.0)\n",
      "  Using cached xgboost-1.6.2-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (1.7 MB)\n",
      "Collecting zipp==3.15.0 (from lohrasb==4.1.0)\n",
      "  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Using cached alembic-1.12.0-py3-none-any.whl (226 kB)\n",
      "Using cached argcomplete-3.1.1-py3-none-any.whl (41 kB)\n",
      "Using cached catboost-1.2.1-cp37-cp37m-macosx_11_0_universal2.whl (25.7 MB)\n",
      "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Using cached charset_normalizer-3.2.0-cp37-cp37m-macosx_10_9_x86_64.whl (122 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
      "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
      "Using cached exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Using cached grpcio-1.57.0-cp37-cp37m-macosx_10_10_universal2.whl (9.0 MB)\n",
      "Using cached imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
      "Using cached importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached kiwisolver-1.4.5-cp37-cp37m-macosx_10_9_x86_64.whl (67 kB)\n",
      "Using cached lightgbm-4.0.0-py3-none-macosx_10_15_x86_64.macosx_11_6_x86_64.macosx_12_5_x86_64.whl (1.7 MB)\n",
      "Using cached MarkupSafe-2.1.3-cp37-cp37m-macosx_10_9_x86_64.whl (13 kB)\n",
      "Using cached optuna-3.3.0-py3-none-any.whl (404 kB)\n",
      "Using cached pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
      "Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB)\n",
      "Using cached pluggy-1.2.0-py3-none-any.whl (17 kB)\n",
      "Using cached protobuf-4.24.2-cp37-abi3-macosx_10_9_universal2.whl (409 kB)\n",
      "Using cached pyarrow-12.0.1-cp37-cp37m-macosx_10_14_x86_64.whl (24.7 MB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Using cached pytest-7.4.1-py3-none-any.whl (324 kB)\n",
      "Using cached PyYAML-6.0.1-cp37-cp37m-macosx_10_9_x86_64.whl (189 kB)\n",
      "Using cached ray-2.6.3-cp37-cp37m-macosx_10_15_x86_64.whl (59.0 MB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
      "Using cached SQLAlchemy-2.0.20-cp37-cp37m-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Using cached tune_sklearn-0.4.6-py3-none-any.whl (41 kB)\n",
      "Using cached typed_ast-1.5.5-cp37-cp37m-macosx_10_9_x86_64.whl (222 kB)\n",
      "Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Using cached urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "Using cached virtualenv-20.24.4-py3-none-any.whl (3.7 MB)\n",
      "Building wheels for collected packages: lohrasb\n",
      "  Building wheel for lohrasb (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lohrasb: filename=lohrasb-4.1.0-py3-none-any.whl size=44130 sha256=8b3e03f21c246a7fb155a3d8b3aea313031eb09347eea071bf743603b6840855\n",
      "  Stored in directory: /private/var/folders/v1/xbcjnd1x5rn7ct1m_rnsblk80000gp/T/pip-ephem-wheel-cache-rcp3up7o/wheels/5a/4b/d5/29692b015a5dec2987d83291222fed5badfc474f476cd7f8b1\n",
      "Successfully built lohrasb\n",
      "Installing collected packages: pytz, py4j, msgpack, distlib, zipp, urllib3, typing_extensions, typed-ast, tqdm, tomli, threadpoolctl, tenacity, six, ruamel.yaml.clib, PyYAML, python-dotenv, pyrsistent, pyparsing, pyflakes, pycodestyle, protobuf, pkgutil_resolve_name, Pillow, pathspec, packaging, numpy, networkx, mypy-extensions, mccabe, MarkupSafe, joblib, isort, iniconfig, idna, grpcio, greenlet, graphviz, future, frozenlist, fonttools, filelock, exceptiongroup, dill, cycler, colorlog, cloudpickle, charset-normalizer, certifi, tensorboardX, scipy, ruamel.yaml, requests, python-dateutil, pyarrow, plotly, platformdirs, kiwisolver, importlib-resources, importlib-metadata, cmaes, aiosignal, xgboost, virtualenv, SQLAlchemy, scikit-learn, pluggy, pandas, matplotlib, Mako, lightgbm, hyperopt, click, attrs, argcomplete, pytest, nox, jsonschema, imbalanced-learn, catboost, black, alembic, ray, optuna, imblearn, tune-sklearn, lohrasb\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3\n",
      "    Uninstalling pytz-2023.3:\n",
      "      Successfully uninstalled pytz-2023.3\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.7\n",
      "    Uninstalling py4j-0.10.9.7:\n",
      "      Successfully uninstalled py4j-0.10.9.7\n",
      "  Attempting uninstall: msgpack\n",
      "    Found existing installation: msgpack 1.0.5\n",
      "    Uninstalling msgpack-1.0.5:\n",
      "      Successfully uninstalled msgpack-1.0.5\n",
      "  Attempting uninstall: distlib\n",
      "    Found existing installation: distlib 0.3.7\n",
      "    Uninstalling distlib-0.3.7:\n",
      "      Successfully uninstalled distlib-0.3.7\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.15.0\n",
      "    Uninstalling zipp-3.15.0:\n",
      "      Successfully uninstalled zipp-3.15.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.4\n",
      "    Uninstalling urllib3-2.0.4:\n",
      "      Successfully uninstalled urllib3-2.0.4\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: typed-ast\n",
      "    Found existing installation: typed-ast 1.5.5\n",
      "    Uninstalling typed-ast-1.5.5:\n",
      "      Successfully uninstalled typed-ast-1.5.5\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: tomli\n",
      "    Found existing installation: tomli 2.0.1\n",
      "    Uninstalling tomli-2.0.1:\n",
      "      Successfully uninstalled tomli-2.0.1\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.1.0\n",
      "    Uninstalling threadpoolctl-3.1.0:\n",
      "      Successfully uninstalled threadpoolctl-3.1.0\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.3\n",
      "    Uninstalling tenacity-8.2.3:\n",
      "      Successfully uninstalled tenacity-8.2.3\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: ruamel.yaml.clib\n",
      "    Found existing installation: ruamel.yaml.clib 0.2.7\n",
      "    Uninstalling ruamel.yaml.clib-0.2.7:\n",
      "      Successfully uninstalled ruamel.yaml.clib-0.2.7\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 0.21.1\n",
      "    Uninstalling python-dotenv-0.21.1:\n",
      "      Successfully uninstalled python-dotenv-0.21.1\n",
      "  Attempting uninstall: pyrsistent\n",
      "    Found existing installation: pyrsistent 0.19.3\n",
      "    Uninstalling pyrsistent-0.19.3:\n",
      "      Successfully uninstalled pyrsistent-0.19.3\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.1\n",
      "    Uninstalling pyparsing-3.1.1:\n",
      "      Successfully uninstalled pyparsing-3.1.1\n",
      "  Attempting uninstall: pyflakes\n",
      "    Found existing installation: pyflakes 2.5.0\n",
      "    Uninstalling pyflakes-2.5.0:\n",
      "      Successfully uninstalled pyflakes-2.5.0\n",
      "  Attempting uninstall: pycodestyle\n",
      "    Found existing installation: pycodestyle 2.9.1\n",
      "    Uninstalling pycodestyle-2.9.1:\n",
      "      Successfully uninstalled pycodestyle-2.9.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.2\n",
      "    Uninstalling protobuf-4.24.2:\n",
      "      Successfully uninstalled protobuf-4.24.2\n",
      "  Attempting uninstall: pkgutil_resolve_name\n",
      "    Found existing installation: pkgutil_resolve_name 1.3.10\n",
      "    Uninstalling pkgutil_resolve_name-1.3.10:\n",
      "      Successfully uninstalled pkgutil_resolve_name-1.3.10\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 9.5.0\n",
      "    Uninstalling Pillow-9.5.0:\n",
      "      Successfully uninstalled Pillow-9.5.0\n",
      "  Attempting uninstall: pathspec\n",
      "    Found existing installation: pathspec 0.11.2\n",
      "    Uninstalling pathspec-0.11.2:\n",
      "      Successfully uninstalled pathspec-0.11.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.6\n",
      "    Uninstalling numpy-1.21.6:\n",
      "      Successfully uninstalled numpy-1.21.6\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.6.3\n",
      "    Uninstalling networkx-2.6.3:\n",
      "      Successfully uninstalled networkx-2.6.3\n",
      "  Attempting uninstall: mypy-extensions\n",
      "    Found existing installation: mypy-extensions 1.0.0\n",
      "    Uninstalling mypy-extensions-1.0.0:\n",
      "      Successfully uninstalled mypy-extensions-1.0.0\n",
      "  Attempting uninstall: mccabe\n",
      "    Found existing installation: mccabe 0.7.0\n",
      "    Uninstalling mccabe-0.7.0:\n",
      "      Successfully uninstalled mccabe-0.7.0\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.3\n",
      "    Uninstalling MarkupSafe-2.1.3:\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.3.2\n",
      "    Uninstalling joblib-1.3.2:\n",
      "      Successfully uninstalled joblib-1.3.2\n",
      "  Attempting uninstall: isort\n",
      "    Found existing installation: isort 5.11.5\n",
      "    Uninstalling isort-5.11.5:\n",
      "      Successfully uninstalled isort-5.11.5\n",
      "  Attempting uninstall: iniconfig\n",
      "    Found existing installation: iniconfig 2.0.0\n",
      "    Uninstalling iniconfig-2.0.0:\n",
      "      Successfully uninstalled iniconfig-2.0.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.57.0\n",
      "    Uninstalling grpcio-1.57.0:\n",
      "      Successfully uninstalled grpcio-1.57.0\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 2.0.2\n",
      "    Uninstalling greenlet-2.0.2:\n",
      "      Successfully uninstalled greenlet-2.0.2\n",
      "  Attempting uninstall: graphviz\n",
      "    Found existing installation: graphviz 0.20.1\n",
      "    Uninstalling graphviz-0.20.1:\n",
      "      Successfully uninstalled graphviz-0.20.1\n",
      "  Attempting uninstall: future\n",
      "    Found existing installation: future 0.18.3\n",
      "    Uninstalling future-0.18.3:\n",
      "      Successfully uninstalled future-0.18.3\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.3.3\n",
      "    Uninstalling frozenlist-1.3.3:\n",
      "      Successfully uninstalled frozenlist-1.3.3\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.38.0\n",
      "    Uninstalling fonttools-4.38.0:\n",
      "      Successfully uninstalled fonttools-4.38.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.12.2\n",
      "    Uninstalling filelock-3.12.2:\n",
      "      Successfully uninstalled filelock-3.12.2\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.1.3\n",
      "    Uninstalling exceptiongroup-1.1.3:\n",
      "      Successfully uninstalled exceptiongroup-1.1.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.11.0\n",
      "    Uninstalling cycler-0.11.0:\n",
      "      Successfully uninstalled cycler-0.11.0\n",
      "  Attempting uninstall: colorlog\n",
      "    Found existing installation: colorlog 6.7.0\n",
      "    Uninstalling colorlog-6.7.0:\n",
      "      Successfully uninstalled colorlog-6.7.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.2.1\n",
      "    Uninstalling cloudpickle-2.2.1:\n",
      "      Successfully uninstalled cloudpickle-2.2.1\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.2.0\n",
      "    Uninstalling charset-normalizer-3.2.0:\n",
      "      Successfully uninstalled charset-normalizer-3.2.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.7.22\n",
      "    Uninstalling certifi-2023.7.22:\n",
      "      Successfully uninstalled certifi-2023.7.22\n",
      "  Attempting uninstall: tensorboardX\n",
      "    Found existing installation: tensorboardX 2.6.2.2\n",
      "    Uninstalling tensorboardX-2.6.2.2:\n",
      "      Successfully uninstalled tensorboardX-2.6.2.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "  Attempting uninstall: ruamel.yaml\n",
      "    Found existing installation: ruamel.yaml 0.17.32\n",
      "    Uninstalling ruamel.yaml-0.17.32:\n",
      "      Successfully uninstalled ruamel.yaml-0.17.32\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 12.0.1\n",
      "    Uninstalling pyarrow-12.0.1:\n",
      "      Successfully uninstalled pyarrow-12.0.1\n",
      "  Attempting uninstall: plotly\n",
      "    Found existing installation: plotly 5.16.1\n",
      "    Uninstalling plotly-5.16.1:\n",
      "      Successfully uninstalled plotly-5.16.1\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.10.0\n",
      "    Uninstalling platformdirs-3.10.0:\n",
      "      Successfully uninstalled platformdirs-3.10.0\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.5\n",
      "    Uninstalling kiwisolver-1.4.5:\n",
      "      Successfully uninstalled kiwisolver-1.4.5\n",
      "  Attempting uninstall: importlib-resources\n",
      "    Found existing installation: importlib-resources 5.12.0\n",
      "    Uninstalling importlib-resources-5.12.0:\n",
      "      Successfully uninstalled importlib-resources-5.12.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.7.0\n",
      "    Uninstalling importlib-metadata-6.7.0:\n",
      "      Successfully uninstalled importlib-metadata-6.7.0\n",
      "  Attempting uninstall: cmaes\n",
      "    Found existing installation: cmaes 0.10.0\n",
      "    Uninstalling cmaes-0.10.0:\n",
      "      Successfully uninstalled cmaes-0.10.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.3.1\n",
      "    Uninstalling aiosignal-1.3.1:\n",
      "      Successfully uninstalled aiosignal-1.3.1\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 1.6.2\n",
      "    Uninstalling xgboost-1.6.2:\n",
      "      Successfully uninstalled xgboost-1.6.2\n",
      "  Attempting uninstall: virtualenv\n",
      "    Found existing installation: virtualenv 20.24.4\n",
      "    Uninstalling virtualenv-20.24.4:\n",
      "      Successfully uninstalled virtualenv-20.24.4\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.20\n",
      "    Uninstalling SQLAlchemy-2.0.20:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.20\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 1.2.0\n",
      "    Uninstalling pluggy-1.2.0:\n",
      "      Successfully uninstalled pluggy-1.2.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.3\n",
      "    Uninstalling matplotlib-3.5.3:\n",
      "      Successfully uninstalled matplotlib-3.5.3\n",
      "  Attempting uninstall: Mako\n",
      "    Found existing installation: Mako 1.2.4\n",
      "    Uninstalling Mako-1.2.4:\n",
      "      Successfully uninstalled Mako-1.2.4\n",
      "  Attempting uninstall: lightgbm\n",
      "    Found existing installation: lightgbm 4.0.0\n",
      "    Uninstalling lightgbm-4.0.0:\n",
      "      Successfully uninstalled lightgbm-4.0.0\n",
      "  Attempting uninstall: hyperopt\n",
      "    Found existing installation: hyperopt 0.2.7\n",
      "    Uninstalling hyperopt-0.2.7:\n",
      "      Successfully uninstalled hyperopt-0.2.7\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: argcomplete\n",
      "    Found existing installation: argcomplete 3.1.1\n",
      "    Uninstalling argcomplete-3.1.1:\n",
      "      Successfully uninstalled argcomplete-3.1.1\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 7.4.1\n",
      "    Uninstalling pytest-7.4.1:\n",
      "      Successfully uninstalled pytest-7.4.1\n",
      "  Attempting uninstall: nox\n",
      "    Found existing installation: nox 2023.4.22\n",
      "    Uninstalling nox-2023.4.22:\n",
      "      Successfully uninstalled nox-2023.4.22\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.17.3\n",
      "    Uninstalling jsonschema-4.17.3:\n",
      "      Successfully uninstalled jsonschema-4.17.3\n",
      "  Attempting uninstall: imbalanced-learn\n",
      "    Found existing installation: imbalanced-learn 0.11.0\n",
      "    Uninstalling imbalanced-learn-0.11.0:\n",
      "      Successfully uninstalled imbalanced-learn-0.11.0\n",
      "  Attempting uninstall: catboost\n",
      "    Found existing installation: catboost 1.2.1\n",
      "    Uninstalling catboost-1.2.1:\n",
      "      Successfully uninstalled catboost-1.2.1\n",
      "  Attempting uninstall: black\n",
      "    Found existing installation: black 23.3.0\n",
      "    Uninstalling black-23.3.0:\n",
      "      Successfully uninstalled black-23.3.0\n",
      "  Attempting uninstall: alembic\n",
      "    Found existing installation: alembic 1.12.0\n",
      "    Uninstalling alembic-1.12.0:\n",
      "      Successfully uninstalled alembic-1.12.0\n",
      "  Attempting uninstall: ray\n",
      "    Found existing installation: ray 2.6.3\n",
      "    Uninstalling ray-2.6.3:\n",
      "      Successfully uninstalled ray-2.6.3\n",
      "  Attempting uninstall: optuna\n",
      "    Found existing installation: optuna 3.3.0\n",
      "    Uninstalling optuna-3.3.0:\n",
      "      Successfully uninstalled optuna-3.3.0\n",
      "  Attempting uninstall: imblearn\n",
      "    Found existing installation: imblearn 0.0\n",
      "    Uninstalling imblearn-0.0:\n",
      "      Successfully uninstalled imblearn-0.0\n",
      "  Attempting uninstall: tune-sklearn\n",
      "    Found existing installation: tune-sklearn 0.4.6\n",
      "    Uninstalling tune-sklearn-0.4.6:\n",
      "      Successfully uninstalled tune-sklearn-0.4.6\n",
      "  Attempting uninstall: lohrasb\n",
      "    Found existing installation: lohrasb 4.1.0\n",
      "    Uninstalling lohrasb-4.1.0:\n",
      "      Successfully uninstalled lohrasb-4.1.0\n",
      "Successfully installed Mako-1.2.4 MarkupSafe-2.1.3 Pillow-9.5.0 PyYAML-6.0.1 SQLAlchemy-2.0.20 aiosignal-1.3.1 alembic-1.12.0 argcomplete-3.1.1 attrs-23.1.0 black-23.3.0 catboost-1.2.1 certifi-2023.7.22 charset-normalizer-3.2.0 click-8.1.7 cloudpickle-2.2.1 cmaes-0.10.0 colorlog-6.7.0 cycler-0.11.0 dill-0.3.7 distlib-0.3.7 exceptiongroup-1.1.3 filelock-3.12.2 fonttools-4.38.0 frozenlist-1.3.3 future-0.18.3 graphviz-0.20.1 greenlet-2.0.2 grpcio-1.57.0 hyperopt-0.2.7 idna-3.4 imbalanced-learn-0.11.0 imblearn-0.0 importlib-metadata-6.7.0 importlib-resources-5.12.0 iniconfig-2.0.0 isort-5.11.5 joblib-1.3.2 jsonschema-4.17.3 kiwisolver-1.4.5 lightgbm-4.0.0 lohrasb-4.1.0 matplotlib-3.5.3 mccabe-0.7.0 msgpack-1.0.5 mypy-extensions-1.0.0 networkx-2.6.3 nox-2023.4.22 numpy-1.21.6 optuna-3.3.0 packaging-23.1 pandas-1.3.5 pathspec-0.11.2 pkgutil_resolve_name-1.3.10 platformdirs-3.10.0 plotly-5.16.1 pluggy-1.2.0 protobuf-4.24.2 py4j-0.10.9.7 pyarrow-12.0.1 pycodestyle-2.9.1 pyflakes-2.5.0 pyparsing-3.1.1 pyrsistent-0.19.3 pytest-7.4.1 python-dateutil-2.8.2 python-dotenv-0.21.1 pytz-2023.3 ray-2.6.3 requests-2.31.0 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 scikit-learn-1.0.2 scipy-1.7.3 six-1.16.0 tenacity-8.2.3 tensorboardX-2.6.2.2 threadpoolctl-3.1.0 tomli-2.0.1 tqdm-4.66.1 tune-sklearn-0.4.6 typed-ast-1.5.5 typing_extensions-4.7.1 urllib3-2.0.4 virtualenv-20.24.4 xgboost-1.6.2 zipp-3.15.0\n",
      "Requirement already satisfied: pandas in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: category_encoders in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (2.6.2)\n",
      "Requirement already satisfied: feature-engine in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: optuna in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (3.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from category_encoders) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from category_encoders) (1.7.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from category_encoders) (0.13.5)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from category_encoders) (0.5.3)\n",
      "Requirement already satisfied: importlib-resources in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from category_encoders) (5.12.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from optuna) (1.12.0)\n",
      "Requirement already satisfied: cmaes>=0.10.0 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from optuna) (0.10.0)\n",
      "Requirement already satisfied: colorlog in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from optuna) (23.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from optuna) (2.0.20)\n",
      "Requirement already satisfied: tqdm in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (6.7.0)\n",
      "Requirement already satisfied: six in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from importlib-resources->category_encoders) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/TorkamaniLab/lohrasb.git --force-reinstall\n",
    "! pip install pandas  category_encoders  feature-engine optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and print some libraries versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages/lohrasb/config.yaml'\n",
      "In this module, the default logging will be applied. The error is [Errno 2] No such file or directory: '/Users/hjavedani/.pyenv/versions/3.7.8/lib/python3.7/site-packages/lohrasb/config.yaml' which will be skipped!\n",
      "default logger setting is applied !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-03 13:02:01,079\tINFO util.py:90 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-09-03 13:02:02,059\tINFO util.py:90 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version : 3.7.8 (default, Feb 27 2023, 18:11:31) \n",
      "[Clang 14.0.0 (clang-1400.0.29.202)]\n",
      "lohrasb version : 4.1.0\n",
      "sklearn version : 1.0.2\n",
      "pandas version : 1.3.5\n",
      "numpy version : 1.21.6\n",
      "optuna version : 3.3.0\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys  # For system-related utilities like getting Python version\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np  # Aliased for better readability\n",
    "import pandas as pd  # Aliased for better readability\n",
    "import optuna  # For optimization\n",
    "import sklearn  # Scikit-learn\n",
    "\n",
    "# Scikit-learn specific imports\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Optuna specific imports\n",
    "from optuna.pruners import HyperbandPruner\n",
    "from optuna.samplers._tpe.sampler import TPESampler\n",
    "\n",
    "# Third-party library specific imports for feature engineering\n",
    "from feature_engine.imputation import CategoricalImputer, MeanMedianImputer\n",
    "from category_encoders import OrdinalEncoder\n",
    "\n",
    "# LightGBM specific imports\n",
    "from lightgbm import *  # Ideally, list specific imports instead of '*'\n",
    "\n",
    "# Local (or application-specific) imports\n",
    "import lohrasb\n",
    "from lohrasb.best_estimator import BaseModel\n",
    "from lohrasb.utils.metrics import f1_plus_tn\n",
    "\n",
    "# Print out versions of key libraries\n",
    "print(f'Python version : {sys.version}')\n",
    "print(f'lohrasb version : {lohrasb.__version__}')\n",
    "print(f'sklearn version : {sklearn.__version__}')\n",
    "print(f'pandas version : {pd.__version__}')  # Using the alias\n",
    "print(f'numpy version : {np.__version__}')\n",
    "print(f'optuna version : {optuna.__version__}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1 : Use Adult Data Set (a classification problem)\n",
    "  \n",
    "https://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Use BestModel in sklearn pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country   label  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urldata= \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "# column names\n",
    "col_names=[\"age\", \"workclass\", \"fnlwgt\" , \"education\" ,\"education-num\",\n",
    "\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\n",
    "\"native-country\",\"label\"\n",
    "]\n",
    "# read data\n",
    "data = pd.read_csv(urldata,header=None,names=col_names,sep=',')\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['label']=='<=50K','label']=0\n",
    "data.loc[data['label']==' <=50K','label']=0\n",
    "\n",
    "data.loc[data['label']=='>50K','label']=1\n",
    "data.loc[data['label']==' >50K','label']=1\n",
    "\n",
    "data['label']=data['label'].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns != \"label\"]\n",
    "y = data.loc[:, data.columns == \"label\"]\n",
    "y = y.values.ravel()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# for sample_weights\n",
    "weights = np.ones(len(y_train))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find feature types for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cols =  X_train.select_dtypes(include=['int']).columns.tolist()\n",
    "float_cols =  X_train.select_dtypes(include=['float']).columns.tolist()\n",
    "cat_cols =  X_train.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model and set it argumens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LGBMClassifier()\n",
    "estimator_params = {\n",
    "        \"boosting_type\":[\"gbdt\"],\n",
    "        \"max_depth\": [6,15],\n",
    "        \"learning_rate\":[0.01, 0.1],\n",
    "        \"random_state\":[42],\n",
    "\n",
    "    }\n",
    "\n",
    "kwargs = {  # params for fit method  \n",
    "            'fit_optuna_kwargs' :{\n",
    "            'sample_weight':None,\n",
    "            },\n",
    "            # params for OptunaSearch\n",
    "            'main_optuna_kwargs' : {\n",
    "            'estimator':estimator,\n",
    "            'estimator_params':estimator_params,\n",
    "            'refit':True,\n",
    "            'measure_of_accuracy' :'f1_score(y_true, y_pred,average=\"weighted\")',\n",
    "\n",
    "            },\n",
    "            'train_test_split_kwargs':{\n",
    "                'test_size':.3,\n",
    "                            \n",
    "            },\n",
    "            'study_search_kwargs':{\n",
    "                'storage':None,\n",
    "                'sampler':TPESampler(),\n",
    "                'pruner':HyperbandPruner(),\n",
    "                'study_name':\"example of optuna optimizer\",\n",
    "                'direction':\"maximize\",\n",
    "                'load_if_exists':False,\n",
    "            },\n",
    "            'optimize_kwargs':{\n",
    "                # optuna optimization params\n",
    "                'n_trials':20,\n",
    "                'timeout':600,\n",
    "                'catch':(),\n",
    "                'callbacks':None,\n",
    "                'gc_after_trial':False,\n",
    "                'show_progress_bar':False,\n",
    "            }\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = BaseModel().optimize_by_optuna(\n",
    "            kwargs=kwargs\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipeline =Pipeline([\n",
    "            # int missing values imputers\n",
    "            ('intimputer', MeanMedianImputer(\n",
    "                imputation_method='median', variables=int_cols)),\n",
    "            # category missing values imputers\n",
    "            ('catimputer', CategoricalImputer(variables=cat_cols)),\n",
    "            #\n",
    "            ('catencoder', OrdinalEncoder()),\n",
    "            # classification model\n",
    "            ('obj', obj),\n",
    "\n",
    "\n",
    " ])\n",
    " \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:04,122] A new study created in memory with name: example of optuna optimizer\n",
      "[I 2023-09-03 13:02:04,274] Trial 0 finished with value: 0.8640178613864075 and parameters: {'boosting_type': 'gbdt', 'max_depth': 6, 'learning_rate': 0.09714552853914048, 'random_state': 42}. Best is trial 0 with value: 0.8640178613864075.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:04,425] Trial 1 finished with value: 0.8650304248596747 and parameters: {'boosting_type': 'gbdt', 'max_depth': 14, 'learning_rate': 0.07702256557671801, 'random_state': 42}. Best is trial 1 with value: 0.8650304248596747.\n",
      "[I 2023-09-03 13:02:04,541] Trial 2 finished with value: 0.8676592089151163 and parameters: {'boosting_type': 'gbdt', 'max_depth': 11, 'learning_rate': 0.0627595129928241, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:04,685] Trial 3 finished with value: 0.8579455578833927 and parameters: {'boosting_type': 'gbdt', 'max_depth': 6, 'learning_rate': 0.034872502772002704, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n",
      "[I 2023-09-03 13:02:04,813] Trial 4 finished with value: 0.866122192465431 and parameters: {'boosting_type': 'gbdt', 'max_depth': 14, 'learning_rate': 0.06695344970218178, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:04,921] Trial 5 finished with value: 0.8671689506224992 and parameters: {'boosting_type': 'gbdt', 'max_depth': 10, 'learning_rate': 0.07648022674297933, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n",
      "[I 2023-09-03 13:02:05,045] Trial 6 finished with value: 0.8633860209140406 and parameters: {'boosting_type': 'gbdt', 'max_depth': 7, 'learning_rate': 0.05297887996959303, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:05,170] Trial 7 finished with value: 0.864975283057365 and parameters: {'boosting_type': 'gbdt', 'max_depth': 15, 'learning_rate': 0.033967815508004824, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n",
      "[I 2023-09-03 13:02:05,296] Trial 8 finished with value: 0.866860300916702 and parameters: {'boosting_type': 'gbdt', 'max_depth': 14, 'learning_rate': 0.09643151134772379, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:05,400] Trial 9 finished with value: 0.8638036273505912 and parameters: {'boosting_type': 'gbdt', 'max_depth': 7, 'learning_rate': 0.05208442003457855, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n",
      "[I 2023-09-03 13:02:05,517] Trial 10 finished with value: 0.840227068958231 and parameters: {'boosting_type': 'gbdt', 'max_depth': 11, 'learning_rate': 0.011795887564320387, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:05,627] Trial 11 finished with value: 0.8654473400293128 and parameters: {'boosting_type': 'gbdt', 'max_depth': 10, 'learning_rate': 0.07611939069130716, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n",
      "[I 2023-09-03 13:02:05,743] Trial 12 finished with value: 0.8660311805360866 and parameters: {'boosting_type': 'gbdt', 'max_depth': 10, 'learning_rate': 0.06765122395000564, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:05,905] Trial 13 finished with value: 0.866304078182473 and parameters: {'boosting_type': 'gbdt', 'max_depth': 12, 'learning_rate': 0.08470081390118173, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n",
      "[I 2023-09-03 13:02:06,025] Trial 14 finished with value: 0.86652313533367 and parameters: {'boosting_type': 'gbdt', 'max_depth': 9, 'learning_rate': 0.06168116738893913, 'random_state': 42}. Best is trial 2 with value: 0.8676592089151163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:06,138] Trial 15 finished with value: 0.8682525880450834 and parameters: {'boosting_type': 'gbdt', 'max_depth': 12, 'learning_rate': 0.08538835877736589, 'random_state': 42}. Best is trial 15 with value: 0.8682525880450834.\n",
      "[I 2023-09-03 13:02:06,251] Trial 16 finished with value: 0.8659465639895738 and parameters: {'boosting_type': 'gbdt', 'max_depth': 12, 'learning_rate': 0.09035009077058441, 'random_state': 42}. Best is trial 15 with value: 0.8682525880450834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:06,369] Trial 17 finished with value: 0.8660925348681009 and parameters: {'boosting_type': 'gbdt', 'max_depth': 12, 'learning_rate': 0.08480580778152427, 'random_state': 42}. Best is trial 15 with value: 0.8682525880450834.\n",
      "[I 2023-09-03 13:02:06,480] Trial 18 finished with value: 0.8651860909479012 and parameters: {'boosting_type': 'gbdt', 'max_depth': 13, 'learning_rate': 0.09924888214624819, 'random_state': 42}. Best is trial 15 with value: 0.8682525880450834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3736, number of negative: 11534\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 670\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.244663 -> initscore=-1.127284\n",
      "[LightGBM] [Info] Start training from score -1.127284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:06,595] Trial 19 finished with value: 0.8669494329668511 and parameters: {'boosting_type': 'gbdt', 'max_depth': 9, 'learning_rate': 0.08547094009924186, 'random_state': 42}. Best is trial 15 with value: 0.8682525880450834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 5291, number of negative: 16524\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 693\n",
      "[LightGBM] [Info] Number of data points in the train set: 21815, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242540 -> initscore=-1.138807\n",
      "[LightGBM] [Info] Start training from score -1.138807\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train,y_train)\n",
    "y_preds = pipeline.predict(X_test)\n",
    "pred_labels = np.rint(y_preds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check performance of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score : \n",
      "0.7104033970276008\n",
      "Classification report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92      8196\n",
      "           1       0.77      0.66      0.71      2550\n",
      "\n",
      "    accuracy                           0.87     10746\n",
      "   macro avg       0.84      0.80      0.81     10746\n",
      "weighted avg       0.87      0.87      0.87     10746\n",
      "\n",
      "Confusion matrix : \n",
      "[[7709  487]\n",
      " [ 877 1673]]\n"
     ]
    }
   ],
   "source": [
    "print('F1 score : ')\n",
    "print(f1_score(y_test,pred_labels))\n",
    "print('Classification report : ')\n",
    "print(classification_report(y_test,pred_labels))\n",
    "print('Confusion matrix : ')\n",
    "print(confusion_matrix(y_test,pred_labels))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some estimators have predict_proba method as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[[0.99214398 0.00785602]\n",
      " [0.65613206 0.34386794]\n",
      " [0.38407584 0.61592416]\n",
      " ...\n",
      " [0.71796618 0.28203382]\n",
      " [0.68588175 0.31411825]\n",
      " [0.97986241 0.02013759]]\n"
     ]
    }
   ],
   "source": [
    "y_preds = pipeline.predict_proba(X_test)\n",
    "print(y_preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Another way of using it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform features to make them ready for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline =Pipeline([\n",
    "            # int missing values imputers\n",
    "            ('intimputer', MeanMedianImputer(\n",
    "                imputation_method='median', variables=int_cols)),\n",
    "            # category missing values imputers\n",
    "            ('catimputer', CategoricalImputer(variables=cat_cols)),\n",
    "            #\n",
    "            ('catencoder', OrdinalEncoder()),\n",
    "            # classification model\n",
    "\n",
    " ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=transform_pipeline.fit_transform(X_train,y_train)\n",
    "X_test=transform_pipeline.transform(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:07,167] A new study created in memory with name: example of optuna optimizer\n",
      "[I 2023-09-03 13:02:07,276] Trial 0 finished with value: 0.8685218550766183 and parameters: {'boosting_type': 'gbdt', 'max_depth': 6, 'learning_rate': 0.09352039527574546, 'random_state': 42}. Best is trial 0 with value: 0.8685218550766183.\n",
      "[I 2023-09-03 13:02:07,371] Trial 1 finished with value: 0.8680368355659926 and parameters: {'boosting_type': 'gbdt', 'max_depth': 7, 'learning_rate': 0.09420294565711353, 'random_state': 42}. Best is trial 0 with value: 0.8685218550766183.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:07,492] Trial 2 finished with value: 0.8698592506234561 and parameters: {'boosting_type': 'gbdt', 'max_depth': 12, 'learning_rate': 0.04719097359520171, 'random_state': 42}. Best is trial 2 with value: 0.8698592506234561.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:07,601] Trial 3 finished with value: 0.8497426270277023 and parameters: {'boosting_type': 'gbdt', 'max_depth': 6, 'learning_rate': 0.02202578908052913, 'random_state': 42}. Best is trial 2 with value: 0.8698592506234561.\n",
      "[I 2023-09-03 13:02:07,699] Trial 4 finished with value: 0.8700923074326851 and parameters: {'boosting_type': 'gbdt', 'max_depth': 7, 'learning_rate': 0.0644922038154149, 'random_state': 42}. Best is trial 4 with value: 0.8700923074326851.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:07,807] Trial 5 finished with value: 0.868673305004951 and parameters: {'boosting_type': 'gbdt', 'max_depth': 10, 'learning_rate': 0.037441786479100854, 'random_state': 42}. Best is trial 4 with value: 0.8700923074326851.\n",
      "[I 2023-09-03 13:02:07,912] Trial 6 finished with value: 0.8698969605353196 and parameters: {'boosting_type': 'gbdt', 'max_depth': 15, 'learning_rate': 0.09058244776468585, 'random_state': 42}. Best is trial 4 with value: 0.8700923074326851.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:08,023] Trial 7 finished with value: 0.8683446154348955 and parameters: {'boosting_type': 'gbdt', 'max_depth': 9, 'learning_rate': 0.0503503617069211, 'random_state': 42}. Best is trial 4 with value: 0.8700923074326851.\n",
      "[I 2023-09-03 13:02:08,130] Trial 8 finished with value: 0.8706330040353975 and parameters: {'boosting_type': 'gbdt', 'max_depth': 10, 'learning_rate': 0.08017426990579991, 'random_state': 42}. Best is trial 8 with value: 0.8706330040353975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:08,236] Trial 9 finished with value: 0.8693519269954058 and parameters: {'boosting_type': 'gbdt', 'max_depth': 9, 'learning_rate': 0.04295471203154003, 'random_state': 42}. Best is trial 8 with value: 0.8706330040353975.\n",
      "[I 2023-09-03 13:02:08,353] Trial 10 finished with value: 0.8692919889796312 and parameters: {'boosting_type': 'gbdt', 'max_depth': 13, 'learning_rate': 0.06960945939084873, 'random_state': 42}. Best is trial 8 with value: 0.8706330040353975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001839 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:08,506] Trial 11 finished with value: 0.8677837442833785 and parameters: {'boosting_type': 'gbdt', 'max_depth': 8, 'learning_rate': 0.06814310189908579, 'random_state': 42}. Best is trial 8 with value: 0.8706330040353975.\n",
      "[I 2023-09-03 13:02:08,631] Trial 12 finished with value: 0.8685251331608812 and parameters: {'boosting_type': 'gbdt', 'max_depth': 11, 'learning_rate': 0.070382753992882, 'random_state': 42}. Best is trial 8 with value: 0.8706330040353975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000379 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:08,740] Trial 13 finished with value: 0.868755047114198 and parameters: {'boosting_type': 'gbdt', 'max_depth': 8, 'learning_rate': 0.07897711926583689, 'random_state': 42}. Best is trial 8 with value: 0.8706330040353975.\n",
      "[I 2023-09-03 13:02:08,858] Trial 14 finished with value: 0.8686888265589969 and parameters: {'boosting_type': 'gbdt', 'max_depth': 14, 'learning_rate': 0.05932193896335917, 'random_state': 42}. Best is trial 8 with value: 0.8706330040353975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000682 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:08,979] Trial 15 finished with value: 0.8706981396720325 and parameters: {'boosting_type': 'gbdt', 'max_depth': 11, 'learning_rate': 0.0820590471373236, 'random_state': 42}. Best is trial 15 with value: 0.8706981396720325.\n",
      "[I 2023-09-03 13:02:09,091] Trial 16 finished with value: 0.8695047382317322 and parameters: {'boosting_type': 'gbdt', 'max_depth': 11, 'learning_rate': 0.08065777441075318, 'random_state': 42}. Best is trial 15 with value: 0.8706981396720325.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:09,209] Trial 17 finished with value: 0.8690484089358681 and parameters: {'boosting_type': 'gbdt', 'max_depth': 12, 'learning_rate': 0.0809112121607618, 'random_state': 42}. Best is trial 15 with value: 0.8706981396720325.\n",
      "[I 2023-09-03 13:02:09,319] Trial 18 finished with value: 0.8709103699147239 and parameters: {'boosting_type': 'gbdt', 'max_depth': 10, 'learning_rate': 0.09827309893283051, 'random_state': 42}. Best is trial 18 with value: 0.8709103699147239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3717, number of negative: 11553\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 15270, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.243418 -> initscore=-1.134028\n",
      "[LightGBM] [Info] Start training from score -1.134028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-03 13:02:09,445] Trial 19 finished with value: 0.8698639987336854 and parameters: {'boosting_type': 'gbdt', 'max_depth': 12, 'learning_rate': 0.09760717421166447, 'random_state': 42}. Best is trial 18 with value: 0.8709103699147239.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 5291, number of negative: 16524\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 693\n",
      "[LightGBM] [Info] Number of data points in the train set: 21815, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.242540 -> initscore=-1.138807\n",
      "[LightGBM] [Info] Start training from score -1.138807\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "obj.fit(X_train,y_train)\n",
    "y_pred = obj.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score plus TN : \n",
      "7709.710403397027\n",
      "F1 score : \n",
      "0.7104033970276008\n",
      "Classification report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92      8196\n",
      "           1       0.77      0.66      0.71      2550\n",
      "\n",
      "    accuracy                           0.87     10746\n",
      "   macro avg       0.84      0.80      0.81     10746\n",
      "weighted avg       0.87      0.87      0.87     10746\n",
      "\n",
      "Confusion matrix : \n",
      "[[7709  487]\n",
      " [ 877 1673]]\n"
     ]
    }
   ],
   "source": [
    "f1_plus_tn\n",
    "print('F1 score plus TN : ')\n",
    "print(f1_plus_tn(y_test,pred_labels))\n",
    "print('F1 score : ')\n",
    "print(f1_score(y_test,pred_labels))\n",
    "print('Classification report : ')\n",
    "print(classification_report(y_test,pred_labels))\n",
    "print('Confusion matrix : ')\n",
    "print(confusion_matrix(y_test,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.09827309893283051, max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.get_best_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.09827309893283051, max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.best_estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get fitted search object and its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=18, state=TrialState.COMPLETE, values=[0.8709103699147239], datetime_start=datetime.datetime(2023, 9, 3, 13, 2, 9, 210973), datetime_complete=datetime.datetime(2023, 9, 3, 13, 2, 9, 319547), params={'boosting_type': 'gbdt', 'max_depth': 10, 'learning_rate': 0.09827309893283051, 'random_state': 42}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'boosting_type': CategoricalDistribution(choices=('gbdt',)), 'max_depth': IntDistribution(high=15, log=False, low=6, step=1), 'learning_rate': FloatDistribution(high=0.1, log=False, low=0.01, step=None), 'random_state': IntDistribution(high=42, log=False, low=42, step=1)}, trial_id=18, value=None)\n"
     ]
    }
   ],
   "source": [
    "OptunaObj = obj.get_optimized_object()\n",
    "print(OptunaObj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ced68e88e9060e5bcf2eaa5b2d4f8fc97e4d610a52d347137abf879072ffb6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
